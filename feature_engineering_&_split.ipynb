{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caricamento dei file train_data_labeled.pkl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path_dataframes = \"dataset/dataframes\"\n",
    "file_path = os.path.join(path_dataframes, \"train_data_labeled.pkl\")\n",
    "\n",
    "# Carica il DataFrame usando pd.read_pickle\n",
    "df_train_labeled = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stampo info sul dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenuto del DataFrame:\n",
      "    TIME   P1   P2   P3   P4   P5   P6   P7  Case  Spacecraft  ...  SV3  SV4  \\\n",
      "0  0.000  2.0  2.0  2.0  2.0  2.0  2.0  2.0     1           1  ...  100  100   \n",
      "1  0.001  2.0  2.0  2.0  2.0  2.0  2.0  2.0     1           1  ...  100  100   \n",
      "2  0.002  2.0  2.0  2.0  2.0  2.0  2.0  2.0     1           1  ...  100  100   \n",
      "3  0.003  2.0  2.0  2.0  2.0  2.0  2.0  2.0     1           1  ...  100  100   \n",
      "4  0.004  2.0  2.0  2.0  2.0  2.0  2.0  2.0     1           1  ...  100  100   \n",
      "\n",
      "   BP1  BP2  BP3 BP4 BP5 BP6 BP7 BV1  \n",
      "0   No   No   No  No  No  No  No  No  \n",
      "1   No   No   No  No  No  No  No  No  \n",
      "2   No   No   No  No  No  No  No  No  \n",
      "3   No   No   No  No  No  No  No  No  \n",
      "4   No   No   No  No  No  No  No  No  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "Dimensione del DataFrame:\n",
      "(212577, 23)\n"
     ]
    }
   ],
   "source": [
    "print(\"Contenuto del DataFrame:\")\n",
    "print(df_train_labeled.head())\n",
    "\n",
    "print(\"\\nDimensione del DataFrame:\")\n",
    "print(df_train_labeled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzione che calcola le metriche nel dominio del tempo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def time_domain_metrics(signal):\n",
    "    \"\"\"\n",
    "    Calcola le metriche nel dominio del tempo per un array 1D (signal).\n",
    "    Restituisce un dizionario con i valori calcolati.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    metrics[\"mean\"] = np.mean(signal)\n",
    "    metrics[\"median\"] = np.median(signal)\n",
    "    metrics[\"p25\"] = np.percentile(signal, 25)\n",
    "    metrics[\"p75\"] = np.percentile(signal, 75)\n",
    "    metrics[\"variance\"] = np.var(signal)\n",
    "    metrics[\"line_integral\"] = np.trapezoid(signal)  # approssima l'integrale\n",
    "    metrics[\"min\"] = np.min(signal)\n",
    "    metrics[\"max\"] = np.max(signal)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzione che calcola le metriche nel dominio della frequenza (FFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_domain_metrics(signal, sampling_rate=1000):\n",
    "    \"\"\"\n",
    "    Calcola alcune metriche spettrali (fft) per un array 1D (signal).\n",
    "    Restituisce un dizionario con i valori calcolati.\n",
    "    Puoi estendere questa funzione con SNR, SINAD, ecc.\n",
    "    \"\"\"\n",
    "    fft_result = np.fft.fft(signal)\n",
    "    freqs = np.fft.fftfreq(len(signal), d=1/sampling_rate)\n",
    "    \n",
    "    # Teniamo solo le frequenze positive (>0), escludendo la componente DC\n",
    "    positive_indices = np.where(freqs > 0)\n",
    "    fft_result = fft_result[positive_indices]\n",
    "    freqs = freqs[positive_indices]\n",
    "    \n",
    "    power_spectrum = np.abs(fft_result) ** 2\n",
    "    \n",
    "    # Esempio di metriche spettrali basilari\n",
    "    metrics = {}\n",
    "    metrics[\"peak_value\"] = np.max(power_spectrum)\n",
    "    metrics[\"peak_freq\"] = freqs[np.argmax(power_spectrum)]\n",
    "    metrics[\"sum_power_spectrum\"] = np.sum(power_spectrum)\n",
    "    metrics[\"std_power_spectrum\"] = np.std(power_spectrum)\n",
    "    \n",
    "    # (Facoltativo) Esempio di RMS nel dominio frequenziale\n",
    "    metrics[\"rms_freq\"] = np.sqrt(np.mean(power_spectrum))\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzione per aggregare un singolo Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_case(case_group, sampling_rate=1000):\n",
    "    \"\"\"\n",
    "    case_group: subset del DataFrame per un singolo Case\n",
    "    Ritorna un dizionario con le metriche calcolate per P1..P7.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # Per le colonne che rimangono invariate all'interno dello stesso Case,\n",
    "    # prendiamo il valore dalla prima riga (o dall'ultima, è uguale).\n",
    "    # Aggiungile a piacimento (Spacecraft, Condition, ecc.)\n",
    "    columns_to_keep = [\"Spacecraft\", \"Condition\",\n",
    "                       \"SV1\", \"SV2\", \"SV3\", \"SV4\", \"BP1\", \"BP2\", \"BP3\", \n",
    "                       \"BP4\", \"BP5\", \"BP6\", \"BP7\", \"BV1\"]\n",
    "    for col in columns_to_keep:\n",
    "        if col in case_group.columns:\n",
    "            result[col] = case_group[col].iloc[0]\n",
    "    \n",
    "    # Per ogni colonna di segnale (P1..P7) calcoliamo le metriche\n",
    "    signal_columns = [col for col in case_group.columns \n",
    "                      if col.startswith(\"P\") and col not in columns_to_keep]\n",
    "    \n",
    "    for col in signal_columns:\n",
    "        signal = case_group[col].values\n",
    "        \n",
    "        # Calcolo metriche dominio del tempo\n",
    "        td_metrics = time_domain_metrics(signal)\n",
    "        # Calcolo metriche dominio della frequenza\n",
    "        fd_metrics = frequency_domain_metrics(signal, sampling_rate=sampling_rate)\n",
    "        \n",
    "        # Inseriamo le metriche nel dizionario finale con un prefisso\n",
    "        for k, v in td_metrics.items():\n",
    "            result[f\"{col}_time_{k}\"] = v\n",
    "        for k, v in fd_metrics.items():\n",
    "            result[f\"{col}_freq_{k}\"] = v\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzione per splittare i dati in 3 finestre temporali, per poi aggregare per ogni split, triplicando così i dati di training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_and_aggregate(df, sampling_rate=1000):\n",
    "    \"\"\"\n",
    "    Suddivide ogni Case in 3 finestre temporali e calcola le metriche per ogni finestra.\n",
    "    \"\"\"\n",
    "    window_size = 0.4  # 400ms\n",
    "    windows = [(0, 0.4), (0.4, 0.8), (0.8, 1.2)]\n",
    "    \n",
    "    aggregated_data = []\n",
    "\n",
    "    for case, case_group in df.groupby(\"Case\"):\n",
    "        for i, (start, end) in enumerate(windows):\n",
    "            # Filtra la finestra temporale\n",
    "            window_data = case_group[(case_group[\"TIME\"] >= start) & (case_group[\"TIME\"] < end)]\n",
    "            \n",
    "            if not window_data.empty:\n",
    "                # Aggrega le metriche per questa finestra\n",
    "                agg_result = aggregate_case(window_data, sampling_rate)\n",
    "                agg_result[\"Case\"] = case\n",
    "                agg_result[\"Window_ID\"] = i + 1  # Identificatore della finestra\n",
    "                aggregated_data.append(agg_result)\n",
    "\n",
    "    return pd.DataFrame(aggregated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo il nuovo dataframe di training triplicato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(531, 107)\n",
      "   Spacecraft Condition  SV1  SV2  SV3  SV4 BP1 BP2 BP3 BP4  ...  \\\n",
      "0           1    Normal  100  100  100  100  No  No  No  No  ...   \n",
      "1           1    Normal  100  100  100  100  No  No  No  No  ...   \n",
      "2           1    Normal  100  100  100  100  No  No  No  No  ...   \n",
      "3           1    Normal  100  100  100  100  No  No  No  No  ...   \n",
      "4           1    Normal  100  100  100  100  No  No  No  No  ...   \n",
      "\n",
      "  P7_time_line_integral P7_time_min P7_time_max P7_freq_peak_value  \\\n",
      "0            784.906580    0.017144    5.013518        5020.028779   \n",
      "1            787.373792   -0.002641    5.016796        5610.686996   \n",
      "2            789.525599   -0.003006    5.017115        5325.708422   \n",
      "3            785.214600   -0.002262    4.994830        5071.356039   \n",
      "4            787.175441   -0.003732    4.999298        5609.065086   \n",
      "\n",
      "   P7_freq_peak_freq  P7_freq_sum_power_spectrum  P7_freq_std_power_spectrum  \\\n",
      "0          65.000000                26930.154869                  501.116403   \n",
      "1          65.000000                26048.487157                  508.501184   \n",
      "2          64.837905                26122.284330                  501.853078   \n",
      "3          65.000000                27028.792007                  505.424186   \n",
      "4          65.000000                26860.719421                  520.657365   \n",
      "\n",
      "   P7_freq_rms_freq  Case  Window_ID  \n",
      "0         11.633031     1          1  \n",
      "1         11.441019     1          2  \n",
      "2         11.428535     1          3  \n",
      "3         11.654316     2          1  \n",
      "4         11.618024     2          2  \n",
      "\n",
      "[5 rows x 107 columns]\n",
      "Window_ID\n",
      "1    177\n",
      "2    177\n",
      "3    177\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Applica la funzione di aggregazione con finestre\n",
    "df_aggregated = split_and_aggregate(df_train_labeled)\n",
    "\n",
    "# Mostra il risultato con print\n",
    "print(df_aggregated.shape)\n",
    "print(df_aggregated.head())\n",
    "\n",
    "# Controllo del bilanciamento delle finestre temporali\n",
    "print(df_aggregated['Window_ID'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esporto il df triplicato in csv e in pickle nella cartella dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dati aggregati esportati in: dataset/dataframes\\train_data_aggregated_split.csv\n"
     ]
    }
   ],
   "source": [
    "# Definisci il percorso della cartella di destinazione\n",
    "output_dir = \"dataset/dataframes\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Crea la cartella se non esiste\n",
    "\n",
    "# Definisci il percorso completo per il file CSV\n",
    "output_path = os.path.join(output_dir, \"train_data_aggregated_split.csv\")\n",
    "output_path_pkl = os.path.join(output_dir, \"train_data_aggregated_split.pkl\")\n",
    "\n",
    "# Esporta il DataFrame in CSV\n",
    "df_aggregated.to_csv(output_path, index=False)\n",
    "df_aggregated.to_pickle(output_path_pkl)\n",
    "\n",
    "print(f\"Dati aggregati esportati in: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facciamo la stessa cosa per il test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prendo il dataset di test con le 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataframes = \"dataset/dataframes\"\n",
    "file_path = os.path.join(path_dataframes, \"test_data_labeled.pkl\")\n",
    "\n",
    "# Carica il DataFrame usando pd.read_pickle\n",
    "df_test_labeled = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggreghiamo e split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138, 94)\n",
      "   Spacecraft  P1_time_mean  P1_time_median  P1_time_p25  P1_time_p75  \\\n",
      "0           1      1.984074        1.968528     1.897964     2.057444   \n",
      "1           1      1.984509        1.963438     1.898697     2.057745   \n",
      "2           1      1.984681        1.963766     1.899070     2.057667   \n",
      "3           1      1.984472        1.960340     1.892148     2.062852   \n",
      "4           1      1.985048        1.958879     1.894102     2.063106   \n",
      "\n",
      "   P1_time_variance  P1_time_line_integral  P1_time_min  P1_time_max  \\\n",
      "0          0.089958             791.609870     0.599879     4.409431   \n",
      "1          0.090624             791.774258     0.561861     4.411653   \n",
      "2          0.090409             793.837928     0.561754     4.411926   \n",
      "3          0.093266             791.761654     0.479821     4.100935   \n",
      "4          0.093737             791.969210     0.453900     4.103057   \n",
      "\n",
      "   P1_freq_peak_value  ...  P7_time_line_integral  P7_time_min  P7_time_max  \\\n",
      "0         1187.765081  ...             784.721639    -0.001597     4.957685   \n",
      "1         1202.301221  ...             787.517554    -0.003549     4.959830   \n",
      "2         1228.619720  ...             789.760005    -0.003637     4.960276   \n",
      "3         1256.382067  ...             785.035029     0.054438     5.082534   \n",
      "4         1253.972941  ...             787.510919    -0.002586     5.085373   \n",
      "\n",
      "   P7_freq_peak_value  P7_freq_peak_freq  P7_freq_sum_power_spectrum  \\\n",
      "0         3639.567068           62.50000                25240.991187   \n",
      "1         3945.055858           62.50000                23656.785685   \n",
      "2         3979.709606           62.34414                23695.311811   \n",
      "3         4950.370311           65.00000                27089.997640   \n",
      "4         5538.603524           65.00000                25996.746576   \n",
      "\n",
      "   P7_freq_std_power_spectrum  P7_freq_rms_freq  Case  Window_ID  \n",
      "0                  404.252030         11.262289   178          1  \n",
      "1                  391.929474         10.903133   178          2  \n",
      "2                  390.011746         10.884694   178          3  \n",
      "3                  499.897599         11.667504   179          1  \n",
      "4                  504.627198         11.429651   179          2  \n",
      "\n",
      "[5 rows x 94 columns]\n",
      "Window_ID\n",
      "1    46\n",
      "2    46\n",
      "3    46\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Applica la funzione di aggregazione con finestre\n",
    "df_test_aggregated = split_and_aggregate(df_test_labeled)\n",
    "\n",
    "# Mostra il risultato con print\n",
    "print(df_test_aggregated.shape)\n",
    "print(df_test_aggregated.head())\n",
    "\n",
    "# Controllo del bilanciamento delle finestre temporali\n",
    "print(df_test_aggregated['Window_ID'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esporto il df di test triplicato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dati aggregati esportati in: dataset/dataframes\\test_data_aggregated_split.csv\n"
     ]
    }
   ],
   "source": [
    "# Definisci il percorso della cartella di destinazione\n",
    "output_dir = \"dataset/dataframes\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Crea la cartella se non esiste\n",
    "\n",
    "# Definisci il percorso completo per il file CSV\n",
    "output_path = os.path.join(output_dir, \"test_data_aggregated_split.csv\")\n",
    "output_path_pkl = os.path.join(output_dir, \"test_data_aggregated_split.pkl\")\n",
    "\n",
    "# Esporta il DataFrame in CSV\n",
    "df_test_aggregated.to_csv(output_path, index=False)\n",
    "df_test_aggregated.to_pickle(output_path_pkl)\n",
    "\n",
    "print(f\"Dati aggregati esportati in: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
